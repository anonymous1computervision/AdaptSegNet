{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import scipy\n",
    "from scipy import ndimage\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torchvision.models as models\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import data, model_zoo\n",
    "# from model.deeplab_multi import Res_Deeplab\n",
    "from model.deeplab_single import Res_Deeplab\n",
    "\n",
    "from dataset.cityscapes_dataset import cityscapesDataSet\n",
    "from collections import OrderedDict\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "from dataset.gta5_dataset import GTA5DataSet\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_MEAN = np.array((104.00698793, 116.66876762, 122.67891434), dtype=np.float32)\n",
    "\n",
    "MODEL = 'DeepLab'\n",
    "BATCH_SIZE = 1\n",
    "ITER_SIZE = 1\n",
    "NUM_WORKERS = 4\n",
    "DATA_DIRECTORY = './data/GTA5'\n",
    "DATA_LIST_PATH = './dataset/gta5_list/train.txt'\n",
    "IGNORE_LABEL = 255\n",
    "INPUT_SIZE = '1280,720'\n",
    "DATA_DIRECTORY_TARGET = './data/Cityscapes/data'\n",
    "DATA_LIST_PATH_TARGET = './dataset/cityscapes_list/train.txt'\n",
    "INPUT_SIZE_TARGET = '1024,512'\n",
    "LEARNING_RATE = 2.5e-4\n",
    "MOMENTUM = 0.9\n",
    "NUM_CLASSES = 19\n",
    "NUM_STEPS = 250000\n",
    "NUM_STEPS_STOP = 100000  # early stopping\n",
    "POWER = 0.9\n",
    "RANDOM_SEED = 1234\n",
    "RESTORE_FROM = 'http://vllab.ucmerced.edu/ytsai/CVPR18/DeepLab_resnet_pretrained_init-f81d91e8.pth'\n",
    "SAVE_NUM_IMAGES = 2\n",
    "SAVE_PRED_EVERY = 5000\n",
    "SNAPSHOT_DIR = './snapshots/'\n",
    "WEIGHT_DECAY = 0.0005\n",
    "\n",
    "LEARNING_RATE_D = 1e-4\n",
    "LAMBDA_SEG = 0.1\n",
    "LAMBDA_ADV_TARGET1 = 0.0002\n",
    "LAMBDA_ADV_TARGET2 = 0.001\n",
    "\n",
    "TARGET = 'cityscapes'\n",
    "SET = 'train'\n",
    "palette = [128, 64, 128, 244, 35, 232, 70, 70, 70, 102, 102, 156, 190, 153, 153, 153, 153, 153, 250, 170, 30,\n",
    "           220, 220, 0, 107, 142, 35, 152, 251, 152, 70, 130, 180, 220, 20, 60, 255, 0, 0, 0, 0, 142, 0, 0, 70,\n",
    "           0, 60, 100, 0, 80, 100, 0, 0, 230, 119, 11, 32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def colorize_mask(mask):\n",
    "    # mask: numpy array of the mask\n",
    "    new_mask = Image.fromarray(mask.astype(np.uint8)).convert('P')\n",
    "    new_mask.putpalette(palette)\n",
    "\n",
    "    return new_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# set arg parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_arguments(args):\n",
    "    \"\"\"Parse all the arguments provided from the CLI.\n",
    "\n",
    "    Returns:\n",
    "      A list of parsed arguments.\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser(description=\"DeepLab-ResNet Network\")\n",
    "    parser.add_argument(\"--data-dir\", type=str, default=DATA_DIRECTORY,\n",
    "                        help=\"Path to the directory containing the Cityscapes dataset.\")\n",
    "    parser.add_argument(\"--data-dir-target\", type=str, default=DATA_DIRECTORY_TARGET,\n",
    "                        help=\"Path to the directory containing the target dataset.\")\n",
    "    parser.add_argument(\"--data-list\", type=str, default=DATA_LIST_PATH,\n",
    "                        help=\"Path to the file listing the images in the dataset.\")\n",
    "    parser.add_argument(\"--model\", type=str, default=MODEL,\n",
    "                        help=\"available options : DeepLab\")\n",
    "    parser.add_argument(\"--target\", type=str, default=TARGET,\n",
    "                        help=\"available options : cityscapes\")\n",
    "    parser.add_argument(\"--batch-size\", type=int, default=BATCH_SIZE,\n",
    "                        help=\"Number of images sent to the network in one step.\")\n",
    "    parser.add_argument(\"--iter-size\", type=int, default=ITER_SIZE,\n",
    "                        help=\"Accumulate gradients for ITER_SIZE iterations.\")\n",
    "    parser.add_argument(\"--num-workers\", type=int, default=NUM_WORKERS,\n",
    "                        help=\"number of workers for multithread dataloading.\")\n",
    "    parser.add_argument(\"--ignore-label\", type=int, default=IGNORE_LABEL,\n",
    "                        help=\"The index of the label to ignore during the training.\")\n",
    "    parser.add_argument(\"--input-size\", type=str, default=INPUT_SIZE,\n",
    "                        help=\"Comma-separated string with height and width of source images.\")\n",
    "    parser.add_argument(\"--data-list-target\", type=str, default=DATA_LIST_PATH_TARGET,\n",
    "                        help=\"Path to the file listing the images in the target dataset.\")\n",
    "    parser.add_argument(\"--input-size-target\", type=str, default=INPUT_SIZE_TARGET,\n",
    "                        help=\"Comma-separated string with height and width of target images.\")\n",
    "    parser.add_argument(\"--is-training\", action=\"store_true\",\n",
    "                        help=\"Whether to updates the running means and variances during the training.\")\n",
    "    parser.add_argument(\"--learning-rate\", type=float, default=LEARNING_RATE,\n",
    "                        help=\"Base learning rate for training with polynomial decay.\")\n",
    "    parser.add_argument(\"--learning-rate-D\", type=float, default=LEARNING_RATE_D,\n",
    "                        help=\"Base learning rate for discriminator.\")\n",
    "    parser.add_argument(\"--lambda-seg\", type=float, default=LAMBDA_SEG,\n",
    "                        help=\"lambda_seg.\")\n",
    "    parser.add_argument(\"--lambda-adv-target1\", type=float, default=LAMBDA_ADV_TARGET1,\n",
    "                        help=\"lambda_adv for adversarial training.\")\n",
    "    parser.add_argument(\"--lambda-adv-target2\", type=float, default=LAMBDA_ADV_TARGET2,\n",
    "                        help=\"lambda_adv for adversarial training.\")\n",
    "    parser.add_argument(\"--momentum\", type=float, default=MOMENTUM,\n",
    "                        help=\"Momentum component of the optimiser.\")\n",
    "    parser.add_argument(\"--not-restore-last\", action=\"store_true\",\n",
    "                        help=\"Whether to not restore last (FC) layers.\")\n",
    "    parser.add_argument(\"--num-classes\", type=int, default=NUM_CLASSES,\n",
    "                        help=\"Number of classes to predict (including background).\")\n",
    "    parser.add_argument(\"--num-steps\", type=int, default=NUM_STEPS,\n",
    "                        help=\"Number of training steps.\")\n",
    "    parser.add_argument(\"--num-steps-stop\", type=int, default=NUM_STEPS_STOP,\n",
    "                        help=\"Number of training steps for early stopping.\")\n",
    "    parser.add_argument(\"--power\", type=float, default=POWER,\n",
    "                        help=\"Decay parameter to compute the learning rate.\")\n",
    "    parser.add_argument(\"--random-mirror\", action=\"store_true\",\n",
    "                        help=\"Whether to randomly mirror the inputs during the training.\")\n",
    "    parser.add_argument(\"--random-scale\", action=\"store_true\",\n",
    "                        help=\"Whether to randomly scale the inputs during the training.\")\n",
    "    parser.add_argument(\"--random-seed\", type=int, default=RANDOM_SEED,\n",
    "                        help=\"Random seed to have reproducible results.\")\n",
    "    parser.add_argument(\"--restore-from\", type=str, default=RESTORE_FROM,\n",
    "                        help=\"Where restore model parameters from.\")\n",
    "    parser.add_argument(\"--save-num-images\", type=int, default=SAVE_NUM_IMAGES,\n",
    "                        help=\"How many images to save.\")\n",
    "    parser.add_argument(\"--save-pred-every\", type=int, default=SAVE_PRED_EVERY,\n",
    "                        help=\"Save summaries and checkpoint every often.\")\n",
    "    parser.add_argument(\"--snapshot-dir\", type=str, default=SNAPSHOT_DIR,\n",
    "                        help=\"Where to save snapshots of the model.\")\n",
    "    parser.add_argument(\"--weight-decay\", type=float, default=WEIGHT_DECAY,\n",
    "                        help=\"Regularisation parameter for L2-loss.\")\n",
    "    parser.add_argument(\"--gpu\", type=int, default=0,\n",
    "                        help=\"choose gpu device.\")\n",
    "    parser.add_argument(\"--set\", type=str, default=SET,\n",
    "                        help=\"choose adaptation set.\")\n",
    "    return parser.parse_args(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = set_arguments(\"--restore-from E:/AdaptionSeg-v31-20180807-single-DA-sp-fastphoto-45k-acc-43/snapshots/GTA2Cityscapes_multi/GTA5_45000.pth --snapshot-dir ./snapshots/GTA2Cityscapes_multi --lambda-seg 0.1 --lambda-adv-target1 0.0002 --lambda-adv-target2 0.001\".split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(batch_size=1, data_dir='./data/GTA5', data_dir_target='./data/Cityscapes/data', data_list='./dataset/gta5_list/train.txt', data_list_target='./dataset/cityscapes_list/train.txt', gpu=0, ignore_label=255, input_size='1280,720', input_size_target='1024,512', is_training=False, iter_size=1, lambda_adv_target1=0.0002, lambda_adv_target2=0.001, lambda_seg=0.1, learning_rate=0.00025, learning_rate_D=0.0001, model='DeepLab', momentum=0.9, not_restore_last=False, num_classes=19, num_steps=250000, num_steps_stop=100000, num_workers=4, power=0.9, random_mirror=False, random_scale=False, random_seed=1234, restore_from='E:/AdaptionSeg-v31-20180807-single-DA-sp-fastphoto-45k-acc-43/snapshots/GTA2Cityscapes_multi/GTA5_45000.pth', save_num_images=2, save_pred_every=5000, set='train', snapshot_dir='./snapshots/GTA2Cityscapes_multi', target='cityscapes', weight_decay=0.0005)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# input and resize setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = (1280, 720)\n",
    "input_size_target = (1024,512)\n",
    "interp = nn.Upsample(size=(input_size[1], input_size[0]), align_corners=False, mode='nearest')\n",
    "interp_target = nn.Upsample(size=(input_size_target[1], input_size_target[0]), mode='nearest')\n",
    "interp_down = nn.Upsample(size=(input_size_target[1]/4, input_size_target[0]/4), mode='nearest')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# deeplab model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Create the model and start the evaluation process.\"\"\"\n",
    "gpu0 = args.gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=True)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (6): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (7): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (8): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (9): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (10): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (11): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (12): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (13): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (14): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (15): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (16): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (17): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (18): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (19): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (20): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (21): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (22): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "  )\n",
       "  (layer6): Classifier_Module(\n",
       "    (conv2d_list): ModuleList(\n",
       "      (0): Conv2d(2048, 19, kernel_size=(3, 3), stride=(1, 1), padding=(6, 6), dilation=(6, 6))\n",
       "      (1): Conv2d(2048, 19, kernel_size=(3, 3), stride=(1, 1), padding=(12, 12), dilation=(12, 12))\n",
       "      (2): Conv2d(2048, 19, kernel_size=(3, 3), stride=(1, 1), padding=(18, 18), dilation=(18, 18))\n",
       "      (3): Conv2d(2048, 19, kernel_size=(3, 3), stride=(1, 1), padding=(24, 24), dilation=(24, 24))\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Res_Deeplab(num_classes=args.num_classes)\n",
    "\n",
    "if args.restore_from[:4] == 'http' :\n",
    "    saved_state_dict = model_zoo.load_url(args.restore_from)\n",
    "else:\n",
    "    saved_state_dict = torch.load(args.restore_from)\n",
    "model.load_state_dict(saved_state_dict)\n",
    "\n",
    "model.eval()\n",
    "model.cuda(gpu0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = data.DataLoader(\n",
    "    GTA5DataSet(args.data_dir, args.data_list, max_iters=args.num_steps * args.iter_size * args.batch_size,\n",
    "                crop_size=input_size,\n",
    "                scale=args.random_scale, mirror=args.random_mirror, mean=IMG_MEAN),\n",
    "    batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./dataset/gta5_list/train.txt'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetloader = data.DataLoader(cityscapesDataSet(args.data_dir_target, args.data_list_target,\n",
    "                                                     max_iters=args.num_steps * args.iter_size * args.batch_size,\n",
    "                                                     crop_size=input_size_target,\n",
    "                                                     scale=False, mirror=args.random_mirror, mean=IMG_MEAN,\n",
    "                                                     set=args.set),\n",
    "                                   batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers,\n",
    "                                   pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader_gen = enumerate(trainloader)\n",
    "targetloader_gen = enumerate(targetloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# trainer source input shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "index, batch = next(trainloader_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels, _, name = batch\n",
    "images = Variable(images).cuda(args.gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 720, 1280])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "Img = labels[:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 10.,  10.,  10.,  ...,  10.,  10.,  10.],\n",
       "         [ 10.,  10.,  10.,  ...,  10.,  10.,  10.],\n",
       "         [ 10.,  10.,  10.,  ...,  10.,  10.,  10.],\n",
       "         ...,\n",
       "         [  0.,   0.,   0.,  ..., 255., 255., 255.],\n",
       "         [  0.,   0.,   0.,  ..., 255., 255., 255.],\n",
       "         [  0.,   0.,   0.,  ..., 255., 255., 255.]]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pilTrans = transforms.ToPILImage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pilImg = pilTrans(Img/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  get boundary map from instance map\n",
    "def label_get_edges(t):\n",
    "    edge = torch.cuda.ByteTensor(t.size()).zero_()\n",
    "    print(\"edge shape\", edge.shape)\n",
    "    edge[:,:,:,1:] = edge[:,:,:,1:] | (t[:,:,:,1:] != t[:,:,:,:-1])\n",
    "    edge[:,:,:,:-1] = edge[:,:,:,:-1] | (t[:,:,:,1:] != t[:,:,:,:-1])\n",
    "    edge[:,:,1:,:] = edge[:,:,1:,:] | (t[:,:,1:,:] != t[:,:,:-1,:])\n",
    "    edge[:,:,:-1,:] = edge[:,:,:-1,:] | (t[:,:,1:,:] != t[:,:,:-1,:])\n",
    "    return edge.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_label = Variable(labels).cuda(args.gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "edge shape torch.Size([1, 1, 720, 1280])\n"
     ]
    }
   ],
   "source": [
    "label_edges = label_get_edges(cuda_label.view([1, labels.shape[0], labels.shape[1], labels.shape[2]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_to_PIL(tensor):\n",
    "    pilTrans = transforms.ToPILImage()\n",
    "    image = pilTrans(tensor.cpu().squeeze(0))\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tensor_to_PIL(label_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.save(\"a.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pilTrans = transforms.ToPILImage()\n",
    "pilImg = pilTrans(Img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0]]]], device='cuda:0', dtype=torch.uint8)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "pilTransTensor = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.ToTensor()\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = label_edges.squeeze().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "imgplot = plt.imshow(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "pic should be Tensor or ndarray. Got <class 'numpy.ndarray'>.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-149-e1a08576356c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mimg_pil\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mToPILImage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\viplab\\anaconda3\\envs\\xiao\\lib\\site-packages\\torchvision\\transforms\\transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m         \"\"\"\n\u001b[1;32m--> 110\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_pil_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    111\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\viplab\\anaconda3\\envs\\xiao\\lib\\site-packages\\torchvision\\transforms\\functional.py\u001b[0m in \u001b[0;36mto_pil_image\u001b[1;34m(pic, mode)\u001b[0m\n\u001b[0;32m    101\u001b[0m     \"\"\"\n\u001b[0;32m    102\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_is_numpy_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_is_tensor_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'pic should be Tensor or ndarray. Got {}.'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m     \u001b[0mnpimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpic\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: pic should be Tensor or ndarray. Got <class 'numpy.ndarray'>."
     ]
    }
   ],
   "source": [
    "img_pil = transforms.ToPILImage()(test[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "pic should be PIL Image or ndarray. Got <class 'numpy.ndarray'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-134-5b3ef619f195>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpilImg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpilTransTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel_edges\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\viplab\\anaconda3\\envs\\xiao\\lib\\site-packages\\torchvision\\transforms\\transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\viplab\\anaconda3\\envs\\xiao\\lib\\site-packages\\torchvision\\transforms\\transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m     74\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mConverted\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m         \"\"\"\n\u001b[1;32m---> 76\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\viplab\\anaconda3\\envs\\xiao\\lib\\site-packages\\torchvision\\transforms\\functional.py\u001b[0m in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \"\"\"\n\u001b[0;32m     43\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_is_pil_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_is_numpy_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'pic should be PIL Image or ndarray. Got {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: pic should be PIL Image or ndarray. Got <class 'numpy.ndarray'>"
     ]
    }
   ],
   "source": [
    "pilImg = pilTransTensor(label_edges.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "pic should be PIL Image or ndarray. Got <class 'torch.Tensor'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-133-e05557f8f5ae>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpilImg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpilTransTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel_edges\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\viplab\\anaconda3\\envs\\xiao\\lib\\site-packages\\torchvision\\transforms\\transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\viplab\\anaconda3\\envs\\xiao\\lib\\site-packages\\torchvision\\transforms\\transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m     74\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mConverted\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m         \"\"\"\n\u001b[1;32m---> 76\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\viplab\\anaconda3\\envs\\xiao\\lib\\site-packages\\torchvision\\transforms\\functional.py\u001b[0m in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \"\"\"\n\u001b[0;32m     43\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_is_pil_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_is_numpy_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'pic should be PIL Image or ndarray. Got {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: pic should be PIL Image or ndarray. Got <class 'torch.Tensor'>"
     ]
    }
   ],
   "source": [
    "pilImg = pilTransTensor(label_edges)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "720"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pilImg.height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=1280x720 at 0x269156D7CF8>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pilImg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    output = model(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 19, 91, 161])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\viplab\\anaconda3\\envs\\xiao\\lib\\site-packages\\torch\\nn\\modules\\upsampling.py:122: UserWarning: nn.Upsampling is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.Upsampling is deprecated. Use nn.functional.interpolate instead.\")\n",
      "c:\\users\\viplab\\anaconda3\\envs\\xiao\\lib\\site-packages\\torch\\nn\\functional.py:1961: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode))\n"
     ]
    }
   ],
   "source": [
    "interp = nn.Upsample(size=(1024, 2048), mode='bilinear')\n",
    "output = interp(output).permute(0, 2, 3, 1)\n",
    "_, output = torch.max(output, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "weakly_map = {\"0\":0, \"1\":1, \"2\":2, \"8\":8, \"10\":10, \"11\":11}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 19\n",
    "weakly_map = [0, 1, 2, 8, 10, 11]\n",
    "filter_weakly_map = set(range(num_classes)) - set(weakly_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{3, 4, 5, 6, 7, 9, 12, 13, 14, 15, 16, 17, 18}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_weakly_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 5,  5,  5,  ...,  8,  8,  8],\n",
       "         [ 5,  5,  5,  ...,  8,  8,  8],\n",
       "         [ 5,  5,  5,  ...,  8,  8,  8],\n",
       "         ...,\n",
       "         [ 0,  0,  0,  ..., 13, 13, 13],\n",
       "         [ 0,  0,  0,  ..., 13, 13, 13],\n",
       "         [ 0,  0,  0,  ..., 13, 13, 13]]], device='cuda:0')"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_mask = torch.zeros_like(test)\n",
    "for value in filter_weakly_map:\n",
    "    test[test==value] = 255\n",
    "#     target_mask = target_mask or (test != value) * (test != 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[255, 255, 255,  ...,   8,   8,   8],\n",
       "         [255, 255, 255,  ...,   8,   8,   8],\n",
       "         [255, 255, 255,  ...,   8,   8,   8],\n",
       "         ...,\n",
       "         [  0,   0,   0,  ..., 255, 255, 255],\n",
       "         [  0,   0,   0,  ..., 255, 255, 255],\n",
       "         [  0,   0,   0,  ..., 255, 255, 255]]], device='cuda:0')"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "map_ is only implemented on CPU tensors",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-43a022ee87b0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mweakly_map\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m255\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: map_ is only implemented on CPU tensors"
     ]
    }
   ],
   "source": [
    "test = test.map_(test, lambda x: weakly_map.get(str(x), 255))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.cpu().data[0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.save('check_output/Image_source_domain_seg/%s_label.png' % (1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.ones([1, 19, 3 ,2]) * 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = torch.ones([1, 19, 3, 2]) * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a * b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_z =  torch.zeros(list(labels.shape) + [num_classes], dtype=torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n, c, h, w = [1, 19, 720, 1280]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# can work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = torch.zeros(n, h, w, c)\n",
    "for i in range(num_class):\n",
    "    test[:,:,:,i] = (labels == i) * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[:,:,:,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h, w = map(int, args.input_size_target.split(','))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inter_mini = nn.Upsample(scale_factor=2, mode='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = inter_mini(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[:,:,:,0].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 1280\n",
    "w = 720\n",
    "zero_channel = torch.zeros((1, w, h))\n",
    "labels_channel = torch.zeros((num_class, w, h)).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_channel.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_class):\n",
    "    labels_channel[i] = torch.where(labels == i, input = labels, other = zero_channel).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # re-assign labels to match the format of Cityscapes\n",
    "label_copy = 255 * np.ones(label.shape, dtype=np.float32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = range(19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_label = np.zeros(tuple([len(files)] + list(labels.shape[1:])), dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.random_integers(3, 10, size = (5, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = a.take(np.where(a>5)).assign(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(19):\n",
    "    class_label[i] = np.ones(class_label[class_label ==  i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# revise part\n",
    "# =======================\n",
    "\n",
    "for i in len(self.files):\n",
    "    class_label[i] = label_copy[label == k]\n",
    "# ======================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, _ = torch.max(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(labels == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = predict[target_mask.view(n, h, w, 1).repeat(1, 1, 1, c)].view(-1, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = Variable(labels.long()).cuda(gpu0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = label.transpose(1, 2).transpose(2, 3).contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mask = (label >= 0) * (label !=255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = target[target_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_var = Variable(images, volatile=True)\n",
    "# img_var = nn.no_grad(img_var)\n",
    "output1, output2 = model(img_var.cuda(gpu0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output1.shape, output2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del img_var, output1, output2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# trainer target input shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index, batch = next(targetloader_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, _, name = batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get image sementic segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "img_var = Variable(image, volatile=True)\n",
    "output1, output2 = model(img_var.cuda(gpu0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp_down(output2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = interp(output2).cpu().data[0].numpy()\n",
    "\n",
    "output = output.transpose(1,2,0)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp_down(output2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = interp(output2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_ = output.permute(0, 2, 3, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_argmax_idx = torch.argmax(output_, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_argmax_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_argmax_idx.squeeze().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label.squeeze().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = output_argmax_idx * label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mm(output_argmax_idx.squeeze().float(), label.squeeze().float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_argmax_idx @ label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = output_argmax.data[0].cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def colorize_mask(mask):\n",
    "    # mask: numpy array of the mask\n",
    "    new_mask = Image.fromarray(mask).convert('P')\n",
    "    new_mask.putpalette(palette)\n",
    "\n",
    "    return new_mask\n",
    "\n",
    "\n",
    "get_colorful_image = colorize_mask(output)\n",
    "get_image = Image.fromarray(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_colorful_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = np.asarray(np.argmax(output, axis=2), dtype=np.uint8)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_colorful_image = colorize_mask(output)\n",
    "get_image = Image.fromarray(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_colorful_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del img_var, output1, output2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# discriminator check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.discriminator import FCDiscriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init D\n",
    "model_D1 = FCDiscriminator(num_classes=args.num_classes)\n",
    "model_D2 = FCDiscriminator(num_classes=args.num_classes)\n",
    "# model_D1.train()\n",
    "model_D1.cuda(args.gpu)\n",
    "# model_D2.train()\n",
    "model_D2.cuda(args.gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_target1, pred_target2 = model(Variable(image, volatile=True).cuda(gpu0))\n",
    "pred_target1 = interp_target(pred_target1)\n",
    "pred_target2 = interp_target(pred_target2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_target1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.softmax(pred_target1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = output.transpose(1,2,0)\n",
    "output = np.asarray(np.argmax(output, axis=2), dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_d = model_D1(F.softmax(pred_target1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del pred_target1, pred_target2, output_d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test my discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.xiao_discriminator import XiaoDiscriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "reload(XiaoDiscriminator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init D\n",
    "model_D1 = XiaoDiscriminator(num_classes=args.num_classes)\n",
    "model_D1.cuda(args.gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_target1, pred_target2 = model(Variable(image, volatile=True).cuda(gpu0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_target1 = interp_target(pred_target1)\n",
    "pred_target2 = interp_target(pred_target2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_d = model_D1(F.softmax(pred_target1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del pred_target1, pred_target2, output_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
